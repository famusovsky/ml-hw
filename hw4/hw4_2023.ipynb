{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 4\n",
    "\n",
    "**Warning 1**: You have 2 weeks for this assignemnt.  **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### Decision Trees - 7 points\n",
    "* [Task 1](#task1) (0.5 points)\n",
    "* [Task 2](#task2) (0.5 points)\n",
    "* [Task 3](#task3) (2 points)\n",
    "* [Task 4](#task4) (0.5 points)\n",
    "* [Task 5](#task5) (0.5 points)\n",
    "* [Task 6](#task6) (2 points)\n",
    "* [Task 7](#task7) (0.5 points)\n",
    "* [Task 8](#task8) (0.5 points)\n",
    "\n",
    "#### Ensembles - 3 points\n",
    "* [Task 1](#task2_1) (1 point)\n",
    "* [Task 2](#task2_2) (0.7 points)\n",
    "* [Task 3](#task2_3) (0.5 points)\n",
    "* [Task 4](#task2_4) (0.7 points)\n",
    "* [Task 5](#task2_5) (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.727636Z",
     "start_time": "2023-11-09T17:46:40.045013Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will be implementing decision tree for the regression by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
    "\n",
    "Here you should implement the function `H()` which calculates impurity criterion. We will be training regression tree, and will take mean absolute deviation as impurity criterion.\n",
    "\n",
    "* You cannot use loops\n",
    "* If `y` is empty, the function should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.732847Z",
     "start_time": "2023-11-09T17:46:40.729241Z"
    }
   },
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    \"\"\"\n",
    "    Calculate impurity criterion\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        array of objects target values in the node\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H(R) : float\n",
    "        Impurity in the node (measuread by variance)\n",
    "    \"\"\"\n",
    "    if y.size == 0:\n",
    "        return 0\n",
    "\n",
    "    mean_y = np.mean(y)\n",
    "    absolute_deviation = np.abs(y - mean_y)\n",
    "    mean_absolute_deviation = np.mean(absolute_deviation)\n",
    "\n",
    "    return mean_absolute_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.875430Z",
     "start_time": "2023-11-09T17:46:40.737694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the function\n",
    "assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)\n",
    "assert np.allclose(H(np.array([])), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
    "\n",
    "To find the best split in the node we need to calculate the cost function. Denote: \n",
    "- `R` all the object in the node\n",
    "- `j` index of the feature selected for the split\n",
    "- `t` threshold\n",
    "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
    "\n",
    "We get the following cost function:\n",
    "\n",
    "$$\n",
    "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
    "$$\n",
    "\n",
    "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.877419Z",
     "start_time": "2023-11-09T17:46:40.877407Z"
    }
   },
   "outputs": [],
   "source": [
    "def Q(X, y, j, t):\n",
    "    \"\"\"\n",
    "    Calculate cost function\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        array of objects in the node \n",
    "    y : ndarray\n",
    "        array of target values in the node \n",
    "    j : int\n",
    "        feature index (column in X)\n",
    "    t : float\n",
    "        threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : float\n",
    "        Value of the cost function\n",
    "    \"\"\"   \n",
    "    R = len(y)\n",
    "    \n",
    "    # Split the data into left and right child nodes\n",
    "    R_l = y[X[:, j] <= t]\n",
    "    R_r = y[X[:, j] > t]\n",
    "    \n",
    "    # Calculate impurity for left and right child nodes using H() function\n",
    "    impurity_l = H(R_l)\n",
    "    impurity_r = H(R_r)\n",
    "    \n",
    "    # Calculate the weighted impurity of the children\n",
    "    weighted_impurity = (len(R_l) / R) * impurity_l + (len(R_r) / R) * impurity_r\n",
    "    \n",
    "    # Calculate the information gain (decrease in impurity)\n",
    "    Q = H(y) - weighted_impurity\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
    "\n",
    "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
    "\n",
    "- `best_split`\n",
    "- `grow_tree`\n",
    "- `get_prediction`\n",
    "\n",
    "Also, please add `min_samples_leaf` parameter to your class\n",
    "\n",
    "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.878647Z",
     "start_time": "2023-11-09T17:46:40.878635Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Class for a decision tree node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    right : Node() or None\n",
    "        Right child\n",
    "    right : Node() or None\n",
    "        Left child\n",
    "    threshold: float\n",
    "        \n",
    "    column: int\n",
    "        \n",
    "    depth: int\n",
    "        \n",
    "    prediction: float\n",
    "        prediction of the target value in the node \n",
    "        (average values calculated on a train dataset)\n",
    "    is_terminal:bool\n",
    "        indicates whether it is a terminal node (leaf) or not\n",
    "    \"\"\"    \n",
    "    def __init__(self):        \n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.threshold = None\n",
    "        self.column = None\n",
    "        self.depth = None\n",
    "        self.is_terminal = False\n",
    "        self.prediction = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.is_terminal:\n",
    "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
    "        else:\n",
    "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \\\n",
    "            format(self.column, self.threshold, self.prediction)\n",
    "        return node_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.879812Z",
     "start_time": "2023-11-09T17:46:40.879801Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Class for a Decision Tree Regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int\n",
    "        Max depth of a decision tree.\n",
    "    min_samples_split : int\n",
    "        Minimal number of samples (objects) in a node to make a split.\n",
    "    \"\"\" \n",
    "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "            \n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split in terms of Q of data in a given decision tree node. \n",
    "        Try all features and thresholds. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects in the parent node\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            1D array with the object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        best_split_column : int\n",
    "            Index of the best split column\n",
    "        best_threshold : float\n",
    "            The best split condition.\n",
    "        X_left : ndarray, shape (n_objects_l, n_features)\n",
    "            Objects in the left child\n",
    "        y_left : ndarray, shape (n_objects_l, )\n",
    "            Objects labels in the left child. \n",
    "        X_right : ndarray, shape (n_objects_r, n_features)\n",
    "            Objects in the right child\n",
    "        y_right : ndarray, shape (n_objects_r, )\n",
    "            Objects labels in the right child. \n",
    "        \"\"\"\n",
    "        \n",
    "        best_split_column = None\n",
    "        best_threshold = None\n",
    "        best_cost = H(y)  # without splitting\n",
    "        X_left, y_left, X_right, y_right = None, None, None, None\n",
    "\n",
    "        for j in range(X.shape[1]):  # iterate over features\n",
    "            thresholds = np.unique(X[:, j])\n",
    "            for t in thresholds:\n",
    "                # Split data\n",
    "                R_l = y[X[:, j] <= t]\n",
    "                R_r = y[X[:, j] > t]\n",
    "\n",
    "                # Skip splits that don't meet min_samples_leaf condition\n",
    "                if len(R_l) < self.min_samples_leaf or len(R_r) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                # Calculate cost function\n",
    "                cost = Q(X, y, j, t)\n",
    "\n",
    "                # Update best split if the cost is lower\n",
    "                if cost > best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_split_column = j\n",
    "                    best_threshold = t\n",
    "                    X_left = X[X[:, j] <= t]\n",
    "                    y_left = y[X[:, j] <= t]\n",
    "                    X_right = X[X[:, j] > t]\n",
    "                    y_right = y[X[:, j] > t]\n",
    "\n",
    "        return best_split_column, best_threshold, X_left, y_left, X_right, y_right\n",
    "\n",
    "    \n",
    "    def is_terminal(self, node, y):\n",
    "        \"\"\"\n",
    "        Check terminality conditions based on `max_depth`, \n",
    "        `min_samples_split` parameters for a given node. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node, \n",
    "            \n",
    "        y : ndarray, shape (n_objects, )\n",
    "            Object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Is_termial : bool\n",
    "            If True, node is terminal\n",
    "        \"\"\"\n",
    "        if node.depth >= self.max_depth:    \n",
    "            return True\n",
    "        if len(y) < self.min_samples_split:   \n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def grow_tree(self, node, X, y):\n",
    "        \"\"\"\n",
    "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
    "         - check terminality conditions\n",
    "         - find best split if node is not terminal\n",
    "         - add child nodes to the node\n",
    "         - call the function recursively for the added child nodes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects \n",
    "        y : ndarray, shape (n_objects)\n",
    "            Labels\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.is_terminal(node, y):\n",
    "            node.is_terminal =True\n",
    "            return\n",
    "                \n",
    "        # Find the best split\n",
    "        best_split_column, best_threshold, X_left, y_left, X_right, y_right = self.best_split(X, y)\n",
    "\n",
    "        if best_split_column is None:  # no good split found\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "\n",
    "        # Update node properties with the best split\n",
    "        node.column = best_split_column\n",
    "        node.threshold = best_threshold\n",
    "        node.left = Node()\n",
    "        node.left.depth = node.depth + 1\n",
    "        node.right = Node()\n",
    "        node.right.depth = node.depth + 1\n",
    "\n",
    "        # Recursively grow the tree for the left and right children\n",
    "        self.grow_tree(node.left, X_left, y_left)\n",
    "        self.grow_tree(node.right, X_right, y_right)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor.\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, accept_sparse=False)\n",
    "        self.is_fitted_ = True\n",
    "        \n",
    "        # Initialize the tree (root node)\n",
    "        self.tree_ = Node()                             \n",
    "        self.tree_.depth = 1                            \n",
    "        self.tree_.prediction = np.mean(y)\n",
    "        \n",
    "        # Grow the tree\n",
    "        self.grow_tree(self.tree_, X, y)\n",
    "        return self        \n",
    "    \n",
    "    def get_prediction(self, node, x):\n",
    "        \"\"\"\n",
    "        Get prediction for an object `x`\n",
    "            - Return prediction of the `node` if it is terminal\n",
    "            - Otherwise, recursively call the function to get \n",
    "            predictions of the proper child\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        x : ndarray, shape (n_features,)\n",
    "            Array of feature values of one object.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : float\n",
    "            Prediction for an object x\n",
    "        \"\"\"\n",
    "        if node.is_terminal:\n",
    "            return node.prediction\n",
    "\n",
    "        if x[node.column] <= node.threshold:\n",
    "            return self.get_prediction(node.left, x)\n",
    "        else:\n",
    "            return self.get_prediction(node.right, x)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Get prediction for each object in X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Returns predictions.\n",
    "        \"\"\"\n",
    "        # Check input and that `fit` had been called\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        \n",
    "        # Get predictions\n",
    "        y_predicted = []\n",
    "        for x in X:\n",
    "            y_curr = self.get_prediction(self.tree_, x)\n",
    "            y_predicted.append(y_curr)\n",
    "        return np.array(y_predicted)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.880754Z",
     "start_time": "2023-11-09T17:46:40.880743Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check yourself\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator_checks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_estimator\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcheck_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMyDecisionTreeRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/estimator_checks.py:630\u001b[0m, in \u001b[0;36mcheck_estimator\u001b[0;34m(estimator, generate_only)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m estimator, check \u001b[38;5;129;01min\u001b[39;00m checks_generator():\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m         \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m SkipTest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;66;03m# SkipTest is thrown when pandas can't be imported, or by checks\u001b[39;00m\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;66;03m# that are in the xfail_checks tag\u001b[39;00m\n\u001b[1;32m    634\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;28mstr\u001b[39m(exception), SkipTestWarning)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_testing.py:156\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    155\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/estimator_checks.py:3031\u001b[0m, in \u001b[0;36mcheck_regressors_train\u001b[0;34m(name, regressor_orig, readonly_memmap, X_dtype)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;66;03m# TODO: find out why PLS and CCA fail. RANSAC is random\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;66;03m# and furthermore assumes the presence of outliers, hence\u001b[39;00m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;66;03m# skipped\u001b[39;00m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _safe_tags(regressor, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoor_score\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3031\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m regressor\u001b[38;5;241m.\u001b[39mscore(X, y_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check yourself\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(MyDecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
    "\n",
    "Load boston dataset and split it on the train ($75\\%$) and test ($25\\%$). Fit Decision Tree of **depth 1, 3, 5** and make the following plots for every case:\n",
    "\n",
    "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
    "- Fitted model (tree visualization)\n",
    "\n",
    "Compare `MAE` on train and test. Have trees overfitted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston \n",
    "# It was deleted from sklearn\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CRIM     per capita crime rate by town\n",
    "ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "INDUS    proportion of non-retail business acres per town\n",
    "CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "NOX      nitric oxides concentration (parts per 10 million)\n",
    "RM       average number of rooms per dwelling\n",
    "AGE      proportion of owner-occupied units built prior to 1940\n",
    "DIS      weighted distances to five Boston employment centres\n",
    "RAD      index of accessibility to radial highways\n",
    "TAX      full-value property-tax rate per $10,000\n",
    "PTRATIO  pupil-teacher ratio by town\n",
    "B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "LSTAT    % lower status of the population\n",
    "MEDV     Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.882171Z",
     "start_time": "2023-11-09T17:46:40.882159Z"
    }
   },
   "outputs": [],
   "source": [
    "names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([data.values[::2, :], data.values[1::2, :2]])\n",
    "y = data.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Fit and visualize MyDecisionTreeRegressor with depths 1, 3, and 5\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m]:\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mfit_and_visualize_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 16\u001b[0m, in \u001b[0;36mfit_and_visualize_tree\u001b[0;34m(max_depth, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     13\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Scatter plot of training points\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtree\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Variable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/pyplot.py:3687\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[1;32m   3669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m   3670\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3685\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3686\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PathCollection:\n\u001b[0;32m-> 3687\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3697\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3704\u001b[0m     sci(__ret)\n\u001b[1;32m   3705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4652\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4650\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m   4651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[0;32m-> 4652\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4655\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   4656\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to fit a decision tree, create visualizations, and compare MAE\n",
    "def fit_and_visualize_tree(max_depth, X_train, y_train, X_test, y_test):\n",
    "    # Fit MyDecisionTreeRegressor\n",
    "    tree = MyDecisionTreeRegressor(max_depth=max_depth, min_samples_split=5, min_samples_leaf=2)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Scatter plot of training points\n",
    "    plt.scatter(X_train[:, tree.tree_.column], y_train, label='Training Data')\n",
    "    plt.xlabel(f\"Feature {tree.tree_.column}\")\n",
    "    plt.ylabel(\"Target Variable\")\n",
    "    plt.title(f\"Scatter Plot of Training Points (Depth {max_depth})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize the decision tree\n",
    "    tree_rules = export_text(tree, feature_names=names)\n",
    "    print(f\"Decision Tree Rules (Depth {max_depth}):\\n{tree_rules}\")\n",
    "\n",
    "    # Compare MAE on train and test\n",
    "    y_train_pred = tree.predict(X_train)\n",
    "    y_test_pred = tree.predict(X_test)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"MAE on Train (Depth {max_depth}): {mae_train:.2f}\")\n",
    "    print(f\"MAE on Test (Depth {max_depth}): {mae_test:.2f}\")\n",
    "\n",
    "# Fit and visualize MyDecisionTreeRegressor with depths 1, 3, and 5\n",
    "for depth in [1, 3, 5]:\n",
    "    fit_and_visualize_tree(depth, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
    "\n",
    "Keep working with boston dataset. \n",
    "- Use `GridSearchCV` to find the best hyperparameters among [`max_depth`, `min_samples_leaf`] on 5-Fold cross-validation\n",
    "- Train the model with the best set of hyperparameters on the whole train dataset. \n",
    "- Report `MAE` on test dataset and hyperparameters of the best estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.884202Z",
     "start_time": "2023-11-09T17:46:40.884125Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
    "\n",
    "Recall definition of bias and variance:\n",
    "$$\n",
    "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
    "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
    "$$\n",
    "\n",
    "We wil now use the following algorithm to estimate bias and variance:\n",
    "\n",
    "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
    "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
    "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
    "4. For a given *object* $n$:\n",
    "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
    "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
    "5. Average bias^2 and variance over all the points\n",
    "    \n",
    "**Implement `get_bias_variance` function, using the algorithm above**\n",
    "\n",
    "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.887222Z",
     "start_time": "2023-11-09T17:46:40.887195Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bias_variance(estimator, x, y, n_iter):\n",
    "    \"\"\" \n",
    "    Calculate bias and variance of the `estimator`. \n",
    "    Using a given dataset and bootstrap with `n_iter` samples. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    n_iter: int\n",
    "        Number of samples in \n",
    "    Returns\n",
    "    -------\n",
    "    bias2 : float, \n",
    "        Estiamted squared bias\n",
    "    variance : float, \n",
    "        Estiamted variance\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.888721Z",
     "start_time": "2023-11-09T17:46:40.888706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
    "\n",
    "get_bias_variance(estimator, X_train.values, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
    "\n",
    "Compute bias and variance for the trees with different min_samples_split. Plot how bias and variance change as min_samples_split increases. \n",
    "\n",
    "Comment on what you observe, how does your result correspond to theory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.889695Z",
     "start_time": "2023-11-09T17:46:40.889683Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
    "\n",
    "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance. \n",
    "\n",
    "Answer the following questions:\n",
    " - How bagging should affect bias and variance in theory?\n",
    " - How bias and variance change (if they change) compared to an individual tree in you experiments? \n",
    " - Do your results align with the theory? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.891228Z",
     "start_time": "2023-11-09T17:46:40.891211Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. More Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will be working with [Billionaires Statistics Dataset](https://www.kaggle.com/datasets/nelgiriyewithana/billionaires-statistics-dataset) to solve a classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:47:35.163040Z",
     "start_time": "2023-11-09T17:47:35.075668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>finalWorth</th>\n",
       "      <th>category</th>\n",
       "      <th>personName</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>source</th>\n",
       "      <th>industries</th>\n",
       "      <th>countryOfCitizenship</th>\n",
       "      <th>...</th>\n",
       "      <th>cpi_change_country</th>\n",
       "      <th>gdp_country</th>\n",
       "      <th>gross_tertiary_education_enrollment</th>\n",
       "      <th>gross_primary_education_enrollment_country</th>\n",
       "      <th>life_expectancy_country</th>\n",
       "      <th>tax_revenue_country_country</th>\n",
       "      <th>total_tax_rate_country</th>\n",
       "      <th>population_country</th>\n",
       "      <th>latitude_country</th>\n",
       "      <th>longitude_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>211000</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>74.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>$2,715,518,274,227</td>\n",
       "      <td>65.6</td>\n",
       "      <td>102.5</td>\n",
       "      <td>82.5</td>\n",
       "      <td>24.2</td>\n",
       "      <td>60.7</td>\n",
       "      <td>67059887.0</td>\n",
       "      <td>46.227638</td>\n",
       "      <td>2.213749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>180000</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>51.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Austin</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>$21,427,700,000,000</td>\n",
       "      <td>88.2</td>\n",
       "      <td>101.8</td>\n",
       "      <td>78.5</td>\n",
       "      <td>9.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>328239523.0</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>114000</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>59.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Medina</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>$21,427,700,000,000</td>\n",
       "      <td>88.2</td>\n",
       "      <td>101.8</td>\n",
       "      <td>78.5</td>\n",
       "      <td>9.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>328239523.0</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>107000</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Larry Ellison</td>\n",
       "      <td>78.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Lanai</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>Technology</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>$21,427,700,000,000</td>\n",
       "      <td>88.2</td>\n",
       "      <td>101.8</td>\n",
       "      <td>78.5</td>\n",
       "      <td>9.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>328239523.0</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>106000</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "      <td>Warren Buffett</td>\n",
       "      <td>92.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Omaha</td>\n",
       "      <td>Berkshire Hathaway</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>$21,427,700,000,000</td>\n",
       "      <td>88.2</td>\n",
       "      <td>101.8</td>\n",
       "      <td>78.5</td>\n",
       "      <td>9.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>328239523.0</td>\n",
       "      <td>37.090240</td>\n",
       "      <td>-95.712891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  finalWorth               category                personName   age  \\\n",
       "0     1      211000       Fashion & Retail  Bernard Arnault & family  74.0   \n",
       "1     2      180000             Automotive                 Elon Musk  51.0   \n",
       "2     3      114000             Technology                Jeff Bezos  59.0   \n",
       "3     4      107000             Technology             Larry Ellison  78.0   \n",
       "4     5      106000  Finance & Investments            Warren Buffett  92.0   \n",
       "\n",
       "         country    city              source             industries  \\\n",
       "0         France   Paris                LVMH       Fashion & Retail   \n",
       "1  United States  Austin       Tesla, SpaceX             Automotive   \n",
       "2  United States  Medina              Amazon             Technology   \n",
       "3  United States   Lanai              Oracle             Technology   \n",
       "4  United States   Omaha  Berkshire Hathaway  Finance & Investments   \n",
       "\n",
       "  countryOfCitizenship  ... cpi_change_country           gdp_country  \\\n",
       "0               France  ...                1.1   $2,715,518,274,227    \n",
       "1        United States  ...                7.5  $21,427,700,000,000    \n",
       "2        United States  ...                7.5  $21,427,700,000,000    \n",
       "3        United States  ...                7.5  $21,427,700,000,000    \n",
       "4        United States  ...                7.5  $21,427,700,000,000    \n",
       "\n",
       "  gross_tertiary_education_enrollment  \\\n",
       "0                                65.6   \n",
       "1                                88.2   \n",
       "2                                88.2   \n",
       "3                                88.2   \n",
       "4                                88.2   \n",
       "\n",
       "  gross_primary_education_enrollment_country life_expectancy_country  \\\n",
       "0                                      102.5                    82.5   \n",
       "1                                      101.8                    78.5   \n",
       "2                                      101.8                    78.5   \n",
       "3                                      101.8                    78.5   \n",
       "4                                      101.8                    78.5   \n",
       "\n",
       "  tax_revenue_country_country total_tax_rate_country population_country  \\\n",
       "0                        24.2                   60.7         67059887.0   \n",
       "1                         9.6                   36.6        328239523.0   \n",
       "2                         9.6                   36.6        328239523.0   \n",
       "3                         9.6                   36.6        328239523.0   \n",
       "4                         9.6                   36.6        328239523.0   \n",
       "\n",
       "  latitude_country longitude_country  \n",
       "0        46.227638          2.213749  \n",
       "1        37.090240        -95.712891  \n",
       "2        37.090240        -95.712891  \n",
       "3        37.090240        -95.712891  \n",
       "4        37.090240        -95.712891  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('Billionaires Statistics Dataset.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['selfMade'])\n",
    "X = df.drop('selfMade', axis=1)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task2_1\"></a> (1 point)\n",
    "\n",
    "Let's start with data preprocessing. \n",
    "\n",
    "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice. \n",
    "1. Split dataset into train and test\n",
    "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
    "    - Categorical: Fill missing values and apply one-hot-encoding (if there are many unique values in a column, you can group them by meaning)\n",
    "    - Numeric: Fill missing values\n",
    "    \n",
    "Use `ColumnTranformer` to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
    "\n",
    "```\n",
    "ColumnTransformer([\n",
    "    ('name1', transform1, column_names1),\n",
    "    ('name2', transform2, column_names2)\n",
    "])\n",
    "```\n",
    "\n",
    "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer. \n",
    "    \n",
    "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
    "\n",
    "```\n",
    "double_tranform = make_pipeline(\n",
    "                        transform_1,\n",
    "                        transform_2\n",
    "                        )\n",
    "```\n",
    "\n",
    "P.S. Choose your favourite way to fill missing values. \n",
    "\n",
    "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Billionaires Statistics Dataset.csv')\n",
    "\n",
    "# Drop columns with a lot of missing values or deemed not useful\n",
    "threshold = 0.3\n",
    "df = df.dropna(thresh=(1 - threshold) * df.shape[0], axis=1)\n",
    "\n",
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('selfMade', axis=1), df['selfMade'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Create transformers for categorical and numerical columns\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),  # You can choose another strategy if needed\n",
    "    OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # You can choose another strategy if needed\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# Create the column transformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_transformer, categorical_cols),\n",
    "        ('numeric', numeric_transformer, numeric_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Transform the data\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "X_test = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2_2\"></a> (0.7 points)\n",
    "\n",
    "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
    "    \n",
    "* Choose one classification metric and justify your choice .\n",
    "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
    "* Which model has the best performance? Which models overfit or underfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting: F1 Score - Mean: 0.8624, Std: 0.0067\n",
      "Random Forest: F1 Score - Mean: 0.8740, Std: 0.0048\n",
      "Decision Tree: F1 Score - Mean: 0.8293, Std: 0.0223\n",
      "SVM: F1 Score - Mean: 0.8572, Std: 0.0069\n",
      "Logistic Regression: F1 Score - Mean: 0.8786, Std: 0.0052\n",
      "\n",
      "Best Model: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Increase max_iter for Logistic Regression and add StandardScaler\n",
    "models = {\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Define classification metric\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Calculate class weights for handling class imbalance\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=[0, 1], y=y_train)\n",
    "\n",
    "# Create StandardScaler for Logistic Regression\n",
    "logreg_scaler = StandardScaler(with_mean=False)  # Set with_mean=False for sparse matrices\n",
    "\n",
    "# Compare models using cross-validation\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    if model_name == 'Logistic Regression':\n",
    "        # Apply StandardScaler to Logistic Regression\n",
    "        model = make_pipeline(logreg_scaler, model)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring=scorer)\n",
    "    results[model_name] = scores\n",
    "\n",
    "# Display the cross-validation results using mean F1 score\n",
    "for model_name, scores in results.items():\n",
    "    print(f\"{model_name}: F1 Score - Mean: {scores.mean():.4f}, Std: {scores.std():.4f}\")\n",
    "\n",
    "# Identify the model with the best performance\n",
    "best_model = max(results, key=lambda k: results[k].mean())\n",
    "print(f\"\\nBest Model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   F1 Score    ,             : F1   ,   .\n",
    "-    Logistic regression (F1 score - Mean = 0.8786).\n",
    "-    , Decision Tree    (  Std  Mean),        (  Mean,   Std)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task2_3\"></a> (0.5 points)\n",
    "\n",
    "More Gradient Boosting. You will have to take one of the three popular boosting implementations (xgboost, lightgbm, catboost). Select hyperparameters (number of trees, learning rate, depth) on cross-validation and compare with the methods from the previous task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for XGBoost: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 50}\n",
      "\n",
      "XGBoost: F1 Score - Mean: 0.8606, Std: 0.0052\n",
      "Gradient Boosting outperforms XGBoost.\n",
      "Random Forest outperforms XGBoost.\n",
      "XGBoost outperforms Decision Tree.\n",
      "XGBoost outperforms SVM.\n",
      "Logistic Regression outperforms XGBoost.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Add XGBoost to the models\n",
    "models['XGBoost'] = xgb.XGBClassifier()\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid_xgboost = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 4, 5, 7]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for XGBoost\n",
    "grid_search_xgboost = GridSearchCV(models['XGBoost'], param_grid=param_grid_xgboost, scoring=scorer, cv=StratifiedKFold(n_splits=5))\n",
    "grid_search_xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters for XGBoost\n",
    "best_params_xgboost = grid_search_xgboost.best_params_\n",
    "print(f\"Best Hyperparameters for XGBoost: {best_params_xgboost}\")\n",
    "\n",
    "# Train XGBoost with the best hyperparameters\n",
    "best_xgboost_model = xgb.XGBClassifier(**best_params_xgboost)\n",
    "best_xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of XGBoost\n",
    "xgboost_scores = cross_val_score(best_xgboost_model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring=scorer)\n",
    "print(f\"\\nXGBoost: F1 Score - Mean: {xgboost_scores.mean():.4f}, Std: {xgboost_scores.std():.4f}\")\n",
    "\n",
    "# Compare XGBoost with other models\n",
    "for model_name, scores in results.items():\n",
    "    if model_name != 'XGBoost':\n",
    "        if xgboost_scores.mean() > scores.mean():\n",
    "            print(f\"XGBoost outperforms {model_name}.\")\n",
    "        elif xgboost_scores.mean() < scores.mean():\n",
    "            print(f\"{model_name} outperforms XGBoost.\")\n",
    "        else:\n",
    "            print(f\"{model_name} has similar performance to XGBoost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    XGBoost   Mean  Std Decision Tree  SVM.    XGBoost  Mean,  Std  XGBoost ,   Gradient Boosting     Logistic Regression. ,    XGBoost ,   Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task2_4\"></a> (0.7 points)\n",
    "\n",
    "Now let's train more fancy ensembles:\n",
    "\n",
    "* Bagging with decision trees as base estimators\n",
    "* Bagging with gradient boosting (with large amount of trees, >100) as base estimators\n",
    "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) \n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
    "\n",
    "\n",
    "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
    "\n",
    "Answer the questions:\n",
    "* Which model has the best performance?\n",
    "* Does bagging reduce overfiting of the gradient boosting with large amount of trees? \n",
    "* What is the difference between voting and staking? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with Decision Trees: F1 Score - Mean: 0.8583, Std: 0.0085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Bagging with decision trees\n",
    "bagging_dt_model = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
    "                                     n_estimators=50,\n",
    "                                     random_state=42)\n",
    "bagging_dt_scores = cross_val_score(bagging_dt_model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring=scorer)\n",
    "print(f\"Bagging with Decision Trees: F1 Score - Mean: {bagging_dt_scores.mean():.4f}, Std: {bagging_dt_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with Gradient Boosting: F1 Score - Mean: 0.8672, Std: 0.0089\n"
     ]
    }
   ],
   "source": [
    "# Bagging with gradient boosting\n",
    "bagging_gb_model = BaggingClassifier(estimator=GradientBoostingClassifier(n_estimators=200),\n",
    "                                     n_estimators=10,\n",
    "                                     random_state=42)\n",
    "bagging_gb_scores = cross_val_score(bagging_gb_model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring=scorer)\n",
    "print(f\"Bagging with Gradient Boosting: F1 Score - Mean: {bagging_gb_scores.mean():.4f}, Std: {bagging_gb_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: F1 Score - Mean: 0.8866, Std: 0.0052\n"
     ]
    }
   ],
   "source": [
    "# Voting classifier\n",
    "voting_model = VotingClassifier(estimators=[('gb', GradientBoostingClassifier(n_estimators=200)),\n",
    "                                            ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "                                            ('logreg', make_pipeline(StandardScaler(with_mean=False), LogisticRegression(max_iter=1000)))],\n",
    "                                            voting='soft')\n",
    "voting_scores = cross_val_score(voting_model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring=scorer)\n",
    "print(f\"Voting Classifier: F1 Score - Mean: {voting_scores.mean():.4f}, Std: {voting_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier with Logistic Regression: F1 Score - Mean: 0.8715, Std: 0.0102\n"
     ]
    }
   ],
   "source": [
    "# Stacking classifier with Logistic Regression as a final model\n",
    "stacking_lr_model = StackingClassifier(estimators=[('rf', RandomForestClassifier(n_estimators=100)),\n",
    "                                                   ('gb', GradientBoostingClassifier(n_estimators=200))],\n",
    "                                       final_estimator=make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000)),\n",
    "                                       cv=StratifiedKFold(n_splits=5))\n",
    "stacking_lr_scores = cross_val_score(stacking_lr_model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring=scorer)\n",
    "print(f\"Stacking Classifier with Logistic Regression: F1 Score - Mean: {stacking_lr_scores.mean():.4f}, Std: {stacking_lr_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier with Gradient Boosting: F1 Score - Mean: 0.8732, Std: 0.0119\n"
     ]
    }
   ],
   "source": [
    "# Stacking classifier with Gradient Boosting as a final model\n",
    "stacking_gb_model = StackingClassifier(estimators=[('rf', RandomForestClassifier(n_estimators=100)),\n",
    "                                                    ('logreg', make_pipeline(StandardScaler(with_mean=False), LogisticRegression(max_iter=1000)))],\n",
    "                                       final_estimator=GradientBoostingClassifier(n_estimators=200),\n",
    "                                       cv=StratifiedKFold(n_splits=5))\n",
    "stacking_gb_scores = cross_val_score(stacking_gb_model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring=scorer)\n",
    "print(f\"Stacking Classifier with Gradient Boosting: F1 Score - Mean: {stacking_gb_scores.mean():.4f}, Std: {stacking_gb_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Ensemble Model: Voting Classifier\n"
     ]
    }
   ],
   "source": [
    "# Compare the models\n",
    "ensemble_models = {\n",
    "    'Bagging with Decision Trees': bagging_dt_scores.mean(),\n",
    "    'Bagging with Gradient Boosting': bagging_gb_scores.mean(),\n",
    "    'Voting Classifier': voting_scores.mean(),\n",
    "    'Stacking with Logistic Regression': stacking_lr_scores.mean(),\n",
    "    'Stacking with Gradient Boosting': stacking_gb_scores.mean()\n",
    "}\n",
    "\n",
    "best_ensemble_model = max(ensemble_models, key=ensemble_models.get)\n",
    "print(f\"\\nBest Ensemble Model: {best_ensemble_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    Voting Classifier.\n",
    "-  ,      ,          ,    (0.0089  0.0067).     .\n",
    "What is the difference between voting and stacking?\n",
    "- Voting:  Voting Classifier     ,         .\n",
    "\n",
    "    Stacking:  stacking     ,        - ( ),    ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task2_5\"></a> (0.1 points)\n",
    "\n",
    "Report the test score for the best model, that you were able to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_model.fit(X_train, y_train)\n",
    "y_pred = voting_model.predict(X_test)\n",
    "\n",
    "# Evaluate the F1 score on the test set\n",
    "test_f1_score = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Test: F1 Score - Mean: 0.8903, Std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Voting Classifier Test: F1 Score - Mean: {test_f1_score.mean():.4f}, Std: {test_f1_score.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
